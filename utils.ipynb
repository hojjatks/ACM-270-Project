{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import numpy as np\n",
    "from keras.layers import Input, Lambda, BatchNormalization, Conv1D, Dropout, Add, Multiply, Concatenate\n",
    "from keras.constraints import MaxNorm\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# Size\n",
    "m = 40\n",
    "\n",
    "####################\n",
    "# Neural net utils #\n",
    "####################\n",
    "\n",
    "# to make a periodic padding of a tensor\n",
    "def keras_padding ( v ):\n",
    "\tif isinstance(v, int):\n",
    "\t\tv = (v, v)\n",
    "\tvleft, vright = v\n",
    "\n",
    "\tdef padlayer ( x ):\n",
    "\t\tleftborder = x[..., -vleft:, :]\n",
    "\t\trigthborder = x[..., :vright, :]\n",
    "\t\treturn K.concatenate([leftborder, x, rigthborder], axis=-2)\n",
    "\n",
    "\treturn padlayer\n",
    "\n",
    "# Add an artificial feature (to handle the weights in the cost function)\n",
    "def dummy_feature( x ):\n",
    "\treturn K.concatenate([x,x],axis=-1)\n",
    "\n",
    "class NNPredictor:\n",
    "\tdef __init__ (self,m,archi,\n",
    "\tNtrain=-1,npred=1,nin=1,\n",
    "\tNepochs=10,bilin=False,batchnorm=True,\n",
    "\tweighted=True, reg=None,finetuning=True,\n",
    "\tbatch_size=128,optimizer='Adagrad',patience=100):\n",
    "\t\t\"\"\"\n",
    "\t\tMain class to handle neural nets\n",
    "\t\t:param m: size of the model\n",
    "\t\t:param archi: architecture in form of a dictionnary of tuples (size, filter size, activation, dropout rate)\n",
    "\t\t:param Ntrain: Number of data taken as training (the rest is taken as test)\n",
    "\t\t:param npred: Nummber of forecast steps in the loss function\n",
    "\t\t:param nin: Number of time steps as input\n",
    "\t\t:param Nepochs: Number of epochs during traning\n",
    "\t\t:param bilin: Activate bilinera layer for the first layer\n",
    "\t\t:param batchnorm: Activate a batchnorm layer in input\n",
    "\t\t:param weighted: Use the inverse of diagonal covariance in the loss function (identity otherwise)\n",
    "\t\t:param reg: Regulariation of the last layer\n",
    "\t\t:param finetuning: Fintune the last layer using a linear regression after optimization\n",
    "\t\t:param batch_size: Batch size during the training\n",
    "\t\t:param optimizer: Optimizer used for training\n",
    "\t\t:param patience: Number of epochs to retain the best test score (has an effect only if Ntrain < size of data)\n",
    "\t\t\"\"\"\n",
    "\t\tassert nin==1 or npred==1, 'Time seq both in and out not implemented'\n",
    "\t\tself._m = m\n",
    "\t\tself._archi = archi\n",
    "\t\tself._Ntrain = Ntrain\n",
    "\t\tif np.isnan(npred):\n",
    "\t\t\tnpred = 1\n",
    "\t\tself._npred = int(npred)\n",
    "\t\tself._nin = nin\n",
    "\t\tself._Nepochs = Nepochs\n",
    "\t\tself._bilin = bilin\n",
    "\t\tself._batchnorm = batchnorm\n",
    "\t\tself._batchnorm = batchnorm\n",
    "\t\tself._weighted = weighted\n",
    "\t\tself._batchsize = batch_size\n",
    "\t\tself._optimizer = optimizer\n",
    "\t\tself._verbfit = 1\n",
    "\t\tself._patience = patience\n",
    "\t\tif reg is None:\n",
    "\t\t\tself._reg = 'ridge',0\n",
    "\t\telse:\n",
    "\t\t\tself._reg = reg\n",
    "\t\tself._finetuning = finetuning\n",
    "\t\tself._smodel = self.buildmodels()\n",
    "\n",
    "\tdef buildmodels(self):\n",
    "\t\t\"\"\"\n",
    "\t\tbuid the neuronel model\n",
    "\t\t:return: return a tuple containing:\n",
    "\t\tthe short model,\n",
    "\t\tthe model with dummy output (to handle covariance),\n",
    "\t\tthe recurrent model to handle Nf > 1\n",
    "\t\tAll the three models share the same weights\n",
    "\t\t\"\"\"\n",
    "\t\tborder = int(np.sum(np.array([kern//2 for fil,kern,activ,dropout in self._archi])))\n",
    "\t\txin = Input(shape=(self._m,self._nin))\n",
    "\t\tx3 = None\n",
    "\t\tpadlayer = keras_padding(border)\n",
    "\t\tx = Lambda(padlayer)(xin)\n",
    "\t\tif self._batchnorm:\n",
    "\t\t\tx = BatchNormalization()(x)\n",
    "\t\tbilintodo = self._bilin\n",
    "\t\tfor nfil,nkern,activ,drop in self._archi:\n",
    "\t\t\tif bilintodo: #bilinear layer (only once)\n",
    "\t\t\t\tif drop > 0:  # Add the maxnormvalue\n",
    "\t\t\t\t\tx1 = Conv1D(nfil, nkern, activation=activ, kernel_constraint=maxnorm(3.))(x)\n",
    "\t\t\t\t\tx1 = Dropout(rate=drop)(x1)\n",
    "\t\t\t\t\tx2 = Conv1D(nfil, nkern, activation=activ, kernel_constraint=maxnorm(3.))(x)\n",
    "\t\t\t\t\tx2 = Dropout(rate=drop)(x2)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tx1 = Conv1D(nfil, nkern, activation=activ)(x)\n",
    "\t\t\t\t\tx2 = Conv1D(nfil, nkern, activation=activ)(x)\n",
    "\t\t\t\tx3 = Multiply()([x1,x2])\n",
    "\n",
    "\t\t\tif drop>0: #Add the maxnormvalue\n",
    "\t\t\t\tx = Conv1D(nfil,nkern,activation=activ,kernel_constraint=maxnorm(3.))(x)\n",
    "\t\t\t\tx =  Dropout(rate=drop)(x)\n",
    "\t\t\telse:\n",
    "\t\t\t\tx = Conv1D(nfil,nkern,activation=activ)(x)\n",
    "\n",
    "\t\t\tif bilintodo:\n",
    "\t\t\t\tx = Concatenate()([x, x3])\n",
    "\t\t\t\tbilintodo = False\n",
    "\t\tif self._reg[1]>0:\n",
    "\t\t\tif self._reg[0] == 'ridge':\n",
    "\t\t\t\tdy = Conv1D(1,1,activation='linear',kernel_regularizer=regularizers.l2(self._reg[1]))(x)\n",
    "\t\t\telse:\n",
    "\t\t\t\traise NotImplementedError(self._reg[0],'regularization no implemented')\n",
    "\t\telse:\n",
    "\t\t\tdy = Conv1D(1,1,activation='linear')(x)\n",
    "\t\tsoutput = Add()([xin,dy])\n",
    "\n",
    "\t\tsmodel = Model(xin,soutput)\n",
    "\t\treturn smodel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
